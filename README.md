# ðŸ“˜ Mini Data Pipeline Project  
### *Web Scraping â†’ SQL Storage â†’ Data Cleaning â†’ Anomaly Detection â†’ Reporting*

The goal is to to simulate a small real-world data workflow using a simple website (`books.toscrape.com`), storing the results in a database, cleaning the dataset, analyzing it, and detecting anomalies.

---

## ðŸš€ Project Overview

This project implements an end-to-end data pipeline:

1. **Web Scraping** â€“ extract structured book data from `https://books.toscrape.com`
2. **Database Storage** â€“ save the data into a database (SQlite for testing, PostgreSQL for Docker) 
3. **Data Cleaning & Preprocessing** â€“ handle missing values, duplicates, data types
4. **Exploratory Data Analysis (EDA)** â€“ basic statistics and visualizations
5. **Anomaly Detection** â€“ identify unusual prices or ratings
6. **Reporting & Visualization** â€“ charts and summary plots

---

## ðŸ§° Technology Stack

### Core Languages
- **Python 3** â€“ scraping, pipeline logic, data processing
- **SQL** (PostgreSQL) â€“ structured storage and querying

### Python Libraries

**Data handling**
- `pandas`
- `numpy`

**Web scraping**
- `requests`
- `beautifulsoup4`

**Anomaly detection / ML**
- `scikit-learn`
- `pyod`

**Visualization**
- `matplotlib`
- `seaborn`

**Environment / tooling**
- PostgreSQL (local file, `data/books.db`)
- Jupyter Notebook / JupyterLab
- Git + GitHub

---
 
## âš¡ Quick start (Windows PowerShell)

Run locally in a virtual environment and run the smoke test:

```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -r requirements.txt
pytest -q
```

Run the pipeline manually:

```powershell
python -m src.pipeline
```

Run with Docker Compose:

```powershell
docker compose up --build
```

These commands assume you run them from the project root.


