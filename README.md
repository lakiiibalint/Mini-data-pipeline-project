# ðŸ“˜ Mini Data Pipeline Project  
### *Web Scraping â†’ SQL Storage â†’ Data Cleaning â†’ Anomaly Detection â†’ Reporting*

The goal is to to simulate a small real-world data workflow using a simple website (`books.toscrape.com`), storing the results in a database, cleaning the dataset, analyzing it, and detecting anomalies.

---

## ðŸš€ Project Overview

This project implements an end-to-end data pipeline:

1. **Web Scraping** â€“ extract structured book data from `https://books.toscrape.com`
2. **Database Storage** â€“ save the data into a local SQLite database
3. **Data Cleaning & Preprocessing** â€“ handle missing values, duplicates, data types
4. **Exploratory Data Analysis (EDA)** â€“ basic statistics and visualizations
5. **Anomaly Detection** â€“ identify unusual prices or ratings
6. **Reporting & Visualization** â€“ charts and summary plots

---

## ðŸ§° Technology Stack

### Core Languages
- **Python 3** â€“ scraping, pipeline logic, data processing
- **SQL** (PostgreSQL) â€“ structured storage and querying

### Python Libraries

**Data handling**
- `pandas`
- `numpy`

**Web scraping**
- `requests`
- `beautifulsoup4`

**Anomaly detection / ML**
- `scikit-learn`
- `pyod`

**Visualization**
- `matplotlib`
- `seaborn`

**Environment / tooling**
- PostgreSQL (local file, `data/books.db`)
- Jupyter Notebook / JupyterLab
- Git + GitHub

---

